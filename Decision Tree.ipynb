{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e47b09a",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "#### Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33fc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Imports \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb4a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bbb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e375e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any missing value?\n",
    "\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbbda5e",
   "metadata": {},
   "source": [
    "#### As we have categorical column(Alcohol_Content), we need to convert into numeric data using encoding method.\n",
    "\n",
    "Content is following some order like Low, Med and High. We need to give weight according to the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dee6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']]) \n",
    "\n",
    "df1 = ord_encoder.fit_transform(data[['Alcohol_content']])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9740641",
   "metadata": {},
   "source": [
    "### Override alcohol content column with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Alcohol_content'] = df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06002cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30aa452",
   "metadata": {},
   "source": [
    "## plotting  Heatmap (Correletion matrix)\n",
    "\n",
    "- Let's try to see if we can reduce the features using different techniques\n",
    "- Let's plot heatmap to visualize and find the coefficient of multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa53de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = data.corr().abs() # This code will get the coefficient of one variable vs all other variable\n",
    "\n",
    "plt.figure(figsize = (14,8))\n",
    "sns.heatmap(df_corr, annot = True, annot_kws={'size':10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b7ac48",
   "metadata": {},
   "source": [
    "seems like alcohol_content and alcohol are correlated as per above heat map. But we need more proof."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be697e58",
   "metadata": {},
   "source": [
    "### Make sure they are really correlated (it should follow some trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data.alcohol, data.Alcohol_content)\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Alcohol_content')\n",
    "plt.title('Alcohol vs Alcohol_content')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf8e65",
   "metadata": {},
   "source": [
    "We see clear trend.As and when Alcohol level increases its content also increasing. So we can delete one of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(columns = ['quality','Alcohol_content'])\n",
    "y = data['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192c372",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state =41) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write one function and call as many as times to check accuracy_score of different models\n",
    "def metric_score(clf, x_train, x_test, y_train, y_test, train = True):\n",
    "    if train:\n",
    "        y_pred = clf.predict(x_train)\n",
    "        \n",
    "        print(\"\\n===================Train Result============================\")\n",
    "        \n",
    "        print(f\"Accuracy Score:{accuracy_score(y_train,y_pred) * 100:.2f}%\")\n",
    "    \n",
    "    elif train == False:\n",
    "        pred = clf.predict(x_test)\n",
    "        \n",
    "        print(\"\\n===================Test Result============================\")\n",
    "        \n",
    "        print(f\"Accuracy Score:{accuracy_score(y_test, pred) * 100:.2f}%\")\n",
    "        \n",
    "        print(\"\\n \\n Test Classification Report \\n\", classification_report(y_test, pred, digits = 2))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f69067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88297d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and pass dataset to check train and test score\n",
    "\n",
    "metric_score(clf, x_train, x_test, y_train, y_test, train = True)   # this is for Training Score\n",
    "\n",
    "metric_score(clf, x_train, x_test, y_train, y_test, train = False)   # this is for Test Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23e344",
   "metadata": {},
   "source": [
    "### Let's see how the tree look like(this is nothing to do with algirithm/accuracy). It's just for visualization purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fee31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = list(x.columns)\n",
    "class_name =list(y_train.unique())\n",
    "feature_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "from Ipython.display import Image\n",
    "import pydotplus\n",
    "\n",
    "# create a dot_file which store the tree structure\n",
    "dot_data = export_graphviz(clf, feature_name = feature_name, rounded = True, filled = True)\n",
    "\n",
    "# Draw graph\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_png(\"myTree.png\")\n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca4d61",
   "metadata": {},
   "source": [
    "### Model Confidene/Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da68e6e",
   "metadata": {},
   "source": [
    "#### Let's now try to tune some hyperparameters using the GridSearchCV algorithm.\n",
    "\n",
    "GridSearchCV is a methos used to tune our hypermeters. We can pass different values of hyperparameters as parameters for grid search. It does a exhausive generation of combination of different parameters passed. Using cross validaton score, Grid Search returns the combination of hyperparameter for which the model is performing the best.\n",
    "\n",
    "#### What are Hyper parameters?\n",
    "\n",
    "   DecisionTreeClassifier(Class_weight = Non, criterion = 'gini', max_depth = None, max_feature =None,      \n",
    "                            max_leaf_nodes=None,min_impurity_decrease=0.0,min_impurity_split=None,                                                      min_samples_leaf=1,min_samples_split=2, min_weight_fraction_leaf = 0.0, presort = False,                                    random_state=None, splitter='best')\n",
    "\n",
    "\n",
    "We can see above the decision tree classfier algorithm takes all those parameters which are also known as hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are tuning four important hyperparameters right now, we are passing the different values for both parameters\n",
    "grid_param = {\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_depth': range(10,15),            # The maxium depth of the tree\n",
    "    'min_sample_leaf' : range(2,6),       # The minimum number of samples required to be at a leaf node\n",
    "    'min_sample_split': range(3,8),       # The minimum number of samples required to split an internal node\n",
    "    'max_leaf_nodes': range(5,10)         # Best nodes are defined as relative in impurity. If none then unlimited number of leaf node.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = clf,\n",
    "                          param_grid = grid_param,\n",
    "                          cv = 5,\n",
    "                          n_jobs =-1) # Use all the cores in your system. For performance improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd72706",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240422f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985265a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate DecisionTreeClassifier with new parameter and train\n",
    "clf = DecisionTreeClassifier(criterion ='entropy', min_samples_split = 3,max_depth = 10, max_leaf_nodes=7, min_samples_leaf = 2)\n",
    "\n",
    "#train the model\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and pass the dataset to check train and test score\n",
    "metric_score(clf, x_train, x_test, y_train, y_test, train = True)   # this is for Training Score\n",
    "\n",
    "metric_score(clf, x_train, x_test, y_train, y_test, train = False)   # this is for Test Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91750ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
